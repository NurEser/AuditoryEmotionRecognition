{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common_Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#from Model import TIMNET_Model\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "class Common_Model(object):\n",
    "\n",
    "    def __init__(self, save_path: str = '', name: str = 'Not Specified'):\n",
    "        self.model = None\n",
    "        self.trained = False \n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict(self, samples):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "\n",
    "    def predict_proba(self, samples):\n",
    "        if not self.trained:\n",
    "            sys.stderr.write(\"No Model.\")\n",
    "            sys.exit(-1)\n",
    "        return self.model.predict_proba(samples)\n",
    "\n",
    "    def save_model(self, model_name: str):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Activation, Lambda\n",
    "from tensorflow.keras.layers import Conv1D, SpatialDropout1D,add,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "def Temporal_Aware_Block(x, s, i, activation, nb_filters, kernel_size, dropout_rate=0, name=''):\n",
    "\n",
    "    original_x = x\n",
    "    #1.1\n",
    "    conv_1_1 = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
    "                  dilation_rate=i, padding='causal')(x)\n",
    "    conv_1_1 = BatchNormalization(trainable=True,axis=-1)(conv_1_1)\n",
    "    conv_1_1 =  Activation(activation)(conv_1_1)\n",
    "    output_1_1 =  SpatialDropout1D(dropout_rate)(conv_1_1)\n",
    "    # 2.1\n",
    "    conv_2_1 = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
    "                  dilation_rate=i, padding='causal')(output_1_1)\n",
    "    conv_2_1 = BatchNormalization(trainable=True,axis=-1)(conv_2_1)\n",
    "    conv_2_1 = Activation(activation)(conv_2_1)\n",
    "    output_2_1 =  SpatialDropout1D(dropout_rate)(conv_2_1)\n",
    "    \n",
    "    if original_x.shape[-1] != output_2_1.shape[-1]:\n",
    "        original_x = Conv1D(filters=nb_filters, kernel_size=1, padding='same')(original_x)\n",
    "        \n",
    "    output_2_1 = Lambda(sigmoid)(output_2_1)\n",
    "    F_x = Lambda(lambda x: tf.multiply(x[0], x[1]))([original_x, output_2_1])\n",
    "    return F_x\n",
    "\n",
    "\n",
    "class TIMNET:\n",
    "    def __init__(self,\n",
    "                 nb_filters=64,\n",
    "                 kernel_size=2,\n",
    "                 nb_stacks=1,\n",
    "                 dilations=None,\n",
    "                 activation = \"relu\",\n",
    "                 dropout_rate=0.1,\n",
    "                 return_sequences=True,\n",
    "                 name='TIMNET'):\n",
    "        self.name = name\n",
    "        self.return_sequences = return_sequences\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dilations = dilations\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_filters = nb_filters\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.mask_value=0.\n",
    "\n",
    "        if not isinstance(nb_filters, int):\n",
    "            raise Exception()\n",
    "\n",
    "    def __call__(self, inputs, mask=None):\n",
    "        if self.dilations is None:\n",
    "            self.dilations = 8\n",
    "        forward = inputs\n",
    "        backward = K.reverse(inputs,axes=1)\n",
    "        \n",
    "        print(\"Input Shape=\",inputs.shape)\n",
    "        forward_convd = Conv1D(filters=self.nb_filters,kernel_size=1, dilation_rate=1, padding='causal')(forward)\n",
    "        backward_convd = Conv1D(filters=self.nb_filters,kernel_size=1, dilation_rate=1, padding='causal')(backward)\n",
    "        \n",
    "        final_skip_connection = []\n",
    "        \n",
    "        skip_out_forward = forward_convd\n",
    "        skip_out_backward = backward_convd\n",
    "        \n",
    "        for s in range(self.nb_stacks):\n",
    "            for i in [2 ** i for i in range(self.dilations)]:\n",
    "                skip_out_forward = Temporal_Aware_Block(skip_out_forward, s, i, self.activation,\n",
    "                                                        self.nb_filters,\n",
    "                                                        self.kernel_size, \n",
    "                                                        self.dropout_rate,  \n",
    "                                                        name=self.name)\n",
    "                skip_out_backward = Temporal_Aware_Block(skip_out_backward, s, i, self.activation,\n",
    "                                                        self.nb_filters,\n",
    "                                                        self.kernel_size, \n",
    "                                                        self.dropout_rate,  \n",
    "                                                        name=self.name)\n",
    "                \n",
    "                temp_skip = add([skip_out_forward, skip_out_backward],name = \"biadd_\"+str(i))\n",
    "                temp_skip=GlobalAveragePooling1D()(temp_skip)\n",
    "                temp_skip=tf.expand_dims(temp_skip, axis=1)\n",
    "                final_skip_connection.append(temp_skip)\n",
    "\n",
    "        output_2 = final_skip_connection[0]\n",
    "        for i,item in enumerate(final_skip_connection):\n",
    "            if i==0:\n",
    "                continue\n",
    "            output_2 = K.concatenate([output_2,item],axis=-2)\n",
    "        x = output_2\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIMNET_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.layers import Layer,Dense,Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from Common_Model import Common_Model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "#from TIMNET import TIMNET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smooth_labels(labels, factor=0.1):\n",
    "    # smooth the labels\n",
    "    labels *= (1 - factor)\n",
    "    labels += (factor / labels.shape[1])\n",
    "    return labels\n",
    "\n",
    "class WeightLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(WeightLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[1],1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)  \n",
    "        super(WeightLayer, self).build(input_shape)  \n",
    " \n",
    "    def call(self, x):\n",
    "        tempx = tf.transpose(x,[0,2,1])\n",
    "        x = K.dot(tempx,self.kernel)\n",
    "        x = tf.squeeze(x,axis=-1)\n",
    "        return  x\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[2])\n",
    "    \n",
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    returnval = ex/K.sum(ex, axis=axis, keepdims=True)\n",
    "    print(\"returnval is: \")\n",
    "    print(returnval)\n",
    "    return ex/K.sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "class TIMNET_Model(Common_Model):\n",
    "    def __init__(self, args, input_shape, class_label, **params):\n",
    "        super(TIMNET_Model,self).__init__(**params)\n",
    "        self.args = args\n",
    "        self.data_shape = input_shape\n",
    "        self.num_classes = len(class_label)\n",
    "        self.class_label = class_label\n",
    "        self.matrix = []\n",
    "        self.eva_matrix = []\n",
    "        self.acc = 0\n",
    "        print(\"TIMNET MODEL SHAPE:\",input_shape)\n",
    "    \n",
    "    def create_model(self):\n",
    "        self.inputs=Input(shape = (self.data_shape[0],self.data_shape[1]))\n",
    "        print(self.inputs.shape)\n",
    "        self.multi_decision = TIMNET(nb_filters=self.args.filter_size,\n",
    "                                kernel_size=self.args.kernel_size, \n",
    "                                nb_stacks=self.args.stack_size,\n",
    "                                dilations=self.args.dilation_size,\n",
    "                                dropout_rate=self.args.dropout,\n",
    "                                activation = self.args.activation,\n",
    "                                return_sequences=True, \n",
    "                                name='TIMNET')(self.inputs)\n",
    "        \n",
    "        self.decision = WeightLayer()(self.multi_decision)\n",
    "        \n",
    "        self.predictions = Dense(self.num_classes, activation='softmax')(self.decision)\n",
    "        self.model = Model(inputs = self.inputs, outputs = [ self.predictions])\n",
    "        \n",
    "        self.model.compile(loss = \"categorical_crossentropy\",\n",
    "                           optimizer =Adam(learning_rate=self.args.lr, beta_1=self.args.beta1, beta_2=self.args.beta2, epsilon=1e-8),\n",
    "                           metrics = ['accuracy'])\n",
    "        print(\"Temporal create succes!\")\n",
    "        \n",
    "   \n",
    "   \n",
    "    def test_one_sample(self, sample , framebyframe,path,fold):\n",
    "        # if audio is split to frames framebyframe must be True. \n",
    "        self.create_model()\n",
    "        weight_path= path+'/'+str(self.args.split_fold)+\"-fold_weights_best_\"+str(fold)+\".hdf5\"\n",
    "        self.model.load_weights(weight_path)\n",
    "        if framebyframe:\n",
    "            y_pred_best = self.model.predict(sample)  \n",
    "            transposed_probabilities = np.transpose(y_pred_best)\n",
    "            #for printing out each emotion probability graph seperately\n",
    "            n_classes = len(self.class_label)\n",
    "            fig, axes = plt.subplots(3, 3, figsize=(6, 6))  \n",
    "            axes = axes.flatten()\n",
    "            plt.suptitle(\"Emotion Probabilities\")\n",
    "            plt.xlabel('Samples')\n",
    "            plt.ylabel('Probability')\n",
    "            for i, class_name in enumerate(self.class_label):\n",
    "                m = i % 3\n",
    "                n = i / 3\n",
    "                axes[i].plot(transposed_probabilities[i], label=class_name)\n",
    "                axes[i].set_title(class_name)\n",
    "                axes[i].set_ylim(0, 1)\n",
    "            plt.tight_layout() \n",
    "            plt.savefig('output_single.png')\n",
    "            plt.show()\n",
    "\n",
    "            #for printing out emotions together. \n",
    "            colors = [\"r\", \"c\", \"g\", \"y\", \"m\", \"b\", \"k\", \"orange\"] \n",
    "            plt.figure(figsize=(30,20))\n",
    "            n=100\n",
    "            for i, class_name in enumerate(self.class_label):\n",
    "                plt.plot(np.arange(y_pred_best.shape[0]), transposed_probabilities[i], label=class_name, color=colors[i])\n",
    "                #plt.plot(np.arange(0, y_pred_best.shape[0], n), transposed_probabilities[i][::n], label=class_name, color=colors[i])\n",
    "\n",
    "            plt.xlabel('Sample index')\n",
    "            plt.ylabel('Probabilities')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.legend() \n",
    "            plt.title('Emotion Probabilities for Each Class')\n",
    "            plt.savefig('output.png')\n",
    "            plt.show()\n",
    "\n",
    "        else: \n",
    "            y_pred_best = self.model.predict(np.expand_dims(sample, 0))\n",
    "            print(\"y_predictions are: \")\n",
    "            print(y_pred_best)\n",
    "        \n",
    "        \n",
    "        return y_pred_best  \n",
    "    \n",
    "    def test_two_samples(self, sample1, sample2,path,fold):\n",
    "        self.create_model()\n",
    "        weight_path = path+'/'+str(self.args.split_fold)+\"-fold_weights_best_\"+str(fold)+\".hdf5\"\n",
    "        self.model.load_weights(weight_path)\n",
    "\n",
    "        predictions_speaker1 = self.model.predict(sample1)\n",
    "        predictions_speaker2 = self.model.predict(sample2)\n",
    "\n",
    "        \n",
    "        return predictions_speaker1, predictions_speaker2\n",
    "    \n",
    "    def test_multi_speaker(self, sample_dict, speaker_order,path,fold):\n",
    "        self.create_model()\n",
    "        weight_path = path+'/'+str(self.args.split_fold)+\"-fold_weights_best_\"+str(fold)+\".hdf5\"\n",
    "        self.model.load_weights(weight_path)\n",
    "        predictions_list = []\n",
    "        occurence_dict = {}\n",
    "        for speaker in speaker_order:\n",
    "            if speaker in occurence_dict:\n",
    "                occurence_dict[speaker] += 1\n",
    "            else:\n",
    "                occurence_dict[speaker] = 0\n",
    "            mfcc_list = sample_dict[speaker]  \n",
    "            prediction = self.model.predict(mfcc_list[occurence_dict[speaker]])\n",
    "            predictions_list.append(prediction)\n",
    "        return predictions_list\n",
    "    \n",
    "\n",
    "    def test_original(self, x, y, path, fold):\n",
    "        i=1\n",
    "        kfold = KFold(n_splits=self.args.split_fold, shuffle=True, random_state=self.args.random_seed)\n",
    "        avg_accuracy = 0\n",
    "        avg_loss = 0\n",
    "        x_feats = []\n",
    "        y_labels = []\n",
    "        for train, test in kfold.split(x, y):\n",
    "            self.create_model()\n",
    "            weight_path=path+'/'+str(self.args.split_fold)+\"-fold_weights_best_\"+str(i)+\".hdf5\"\n",
    "            self.model.fit(x[train], y[train],validation_data=(x[test],  y[test]),batch_size = 64,epochs = 0,verbose=0)\n",
    "            self.model.load_weights(weight_path)#+source_name+'_single_best.hdf5')\n",
    "            best_eva_list = self.model.evaluate(x[test],  y[test])\n",
    "        \n",
    "            avg_loss += best_eva_list[0]\n",
    "            avg_accuracy += best_eva_list[1]\n",
    "            print(str(i)+'_Model evaluation: ', best_eva_list,\"   Now ACC:\",str(round(avg_accuracy*10000)/100/i))\n",
    "            i+=1\n",
    "            y_pred_best = self.model.predict(x[test]) \n",
    "        \n",
    "            transposed_probabilities = np.transpose(y_pred_best)\n",
    "            \n",
    "            self.matrix.append(confusion_matrix(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1)))\n",
    "            em = classification_report(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1), target_names=self.class_label,output_dict=True)\n",
    "            self.eva_matrix.append(em)\n",
    "            print(classification_report(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1), target_names=self.class_label))\n",
    "            caps_layer_model = Model(inputs=self.model.input,\n",
    "            outputs=self.model.get_layer(index=-2).output)\n",
    "            feature_source = caps_layer_model.predict(x[test])\n",
    "            x_feats.append(feature_source)\n",
    "            y_labels.append(y[test])\n",
    "            \n",
    "        print(\"Average ACC:\",avg_accuracy/self.args.split_fold)\n",
    "        self.acc = avg_accuracy/self.args.split_fold\n",
    "        \n",
    "        return x_feats, y_labels\n",
    "\n",
    "\n",
    "\n",
    "    def test(self, x, y, path):\n",
    "        i=1\n",
    "        kfold = KFold(n_splits=self.args.split_fold, shuffle=True, random_state=self.args.random_seed)\n",
    "        avg_accuracy = 0\n",
    "        avg_loss = 0\n",
    "        x_feats = []\n",
    "        y_labels = []\n",
    "        for train, test in kfold.split(x, y):\n",
    "            self.create_model()\n",
    "            weight_path=path+'/'+str(self.args.split_fold)+\"-fold_weights_best_\"+str(i)+\".hdf5\"\n",
    "            self.model.fit(x[train], y[train],validation_data=(x[test][0],  y[test][0]),batch_size = 64,epochs = 0,verbose=0)\n",
    "            self.model.load_weights(weight_path)#+source_name+'_single_best.hdf5')\n",
    "            best_eva_list = self.model.evaluate(np.expand_dims(x[test][0], 0), np.expand_dims(y[test][0], 0))\n",
    "            #best_eva_list = self.model.evaluate(x[test][0],  y[test][0])\n",
    "            x_test = x[test]\n",
    "            x_test0 = x_test[0]\n",
    "            avg_loss += best_eva_list[0]\n",
    "            avg_accuracy += best_eva_list[1]\n",
    "            print(str(i)+'_Model evaluation: ', best_eva_list,\"   Now ACC:\",str(round(avg_accuracy*10000)/100/i))\n",
    "            i+=1\n",
    "            y_pred_best = self.model.predict(np.expand_dims(x[test][0], 0))\n",
    "            print(\"y_pred_best is: \")\n",
    "            print(y_pred_best)\n",
    "            print(y[test][0])\n",
    "            break\n",
    "        return x_feats, y_labels\n",
    "    \n",
    "\n",
    "    def train(self, x, y):\n",
    "\n",
    "        filepath = self.args.model_path\n",
    "        resultpath = self.args.result_path\n",
    "\n",
    "        if not os.path.exists(filepath):\n",
    "            os.mkdir(filepath)\n",
    "        if not os.path.exists(resultpath):\n",
    "            os.mkdir(resultpath)\n",
    "\n",
    "        i=1\n",
    "        now = datetime.datetime.now()\n",
    "        now_time = datetime.datetime.strftime(now,'%Y-%m-%d_%H-%M-%S')\n",
    "        kfold = KFold(n_splits=self.args.split_fold, shuffle=True, random_state=self.args.random_seed)\n",
    "        avg_accuracy = 0\n",
    "        avg_loss = 0\n",
    "        for train, test in kfold.split(x, y):\n",
    "            self.create_model()\n",
    "            y[train] = smooth_labels(y[train], 0.1)\n",
    "            folder_address = filepath+self.args.data+\"_\"+str(self.args.random_seed)+\"_\"+now_time\n",
    "            if not os.path.exists(folder_address):\n",
    "                os.mkdir(folder_address)\n",
    "            weight_path=folder_address+'/'+str(self.args.split_fold)+\"-fold_weights_best_\"+str(i)+\".hdf5\"\n",
    "            checkpoint = callbacks.ModelCheckpoint(weight_path, monitor='val_accuracy', verbose=1,save_weights_only=True,save_best_only=True,mode='max')\n",
    "            max_acc = 0\n",
    "            best_eva_list = []\n",
    "            h = self.model.fit(x[train], y[train],validation_data=(x[test],  y[test]),batch_size = self.args.batch_size, epochs = self.args.epoch, verbose=1,callbacks=[checkpoint])\n",
    "            self.model.load_weights(weight_path)\n",
    "            best_eva_list = self.model.evaluate(x[test],  y[test])\n",
    "            avg_loss += best_eva_list[0]\n",
    "            avg_accuracy += best_eva_list[1]\n",
    "            print(str(i)+'_Model evaluation: ', best_eva_list,\"   Now ACC:\",str(round(avg_accuracy*10000)/100/i))\n",
    "            i+=1\n",
    "            y_pred_best = self.model.predict(x[test])\n",
    "            self.matrix.append(confusion_matrix(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1)))\n",
    "            em = classification_report(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1), target_names=self.class_label,output_dict=True)\n",
    "            self.eva_matrix.append(em)\n",
    "            print(classification_report(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1), target_names=self.class_label))\n",
    "\n",
    "        print(\"Average ACC:\",avg_accuracy/self.args.split_fold)\n",
    "        self.acc = avg_accuracy/self.args.split_fold\n",
    "        writer = pd.ExcelWriter(resultpath+self.args.data+'_'+str(self.args.split_fold)+'fold_'+str(round(self.acc*10000)/100)+\"_\"+str(self.args.random_seed)+\"_\"+now_time+'.xlsx')\n",
    "        for i,item in enumerate(self.matrix):\n",
    "            temp = {}\n",
    "            temp[\" \"] = self.class_label\n",
    "            for j,l in enumerate(item):\n",
    "                temp[self.class_label[j]]=item[j]\n",
    "            data1 = pd.DataFrame(temp)\n",
    "            data1.to_excel(writer,sheet_name=str(i), encoding='utf8')\n",
    "\n",
    "            df = pd.DataFrame(self.eva_matrix[i]).transpose()\n",
    "            df.to_excel(writer,sheet_name=str(i)+\"_evaluate\", encoding='utf8')\n",
    "        writer.save()\n",
    "        writer.close()\n",
    "\n",
    "        K.clear_session()\n",
    "        self.matrix = []\n",
    "        self.eva_matrix = []\n",
    "        self.acc = 0\n",
    "        self.trained = True\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import soundfile as sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mfccs(mfccs):\n",
    "    plt.figure(figsize= (10,4))\n",
    "    librosa.display.specshow(mfccs, x_axis = \"time\" )\n",
    "    plt.colorbar(format = \"%+2.0f \")\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_framed(y ,sr, frame_length, skip_length):\n",
    "    frame_length_samples = int(frame_length * sr)\n",
    "    skip_length_samples = int(skip_length * sr)\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    if len(y) < frame_length_samples:\n",
    "        pad_length = frame_length_samples - len(y)\n",
    "        y = np.pad(y, (0, pad_length), 'constant', constant_values=0) \n",
    "        frames.append(y)\n",
    "        return frames, sr\n",
    "    \n",
    "    for i in range(0, len(y) - frame_length_samples, skip_length_samples):\n",
    "        frame = y[i:i+frame_length_samples]\n",
    "        frames.append(frame)\n",
    "\n",
    "    return frames, sr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_frompath(version, file_path = str , mfcc_len: int = 39):\n",
    "  \t  ## \"\"\"\n",
    "      ## file_path: Speech signal folder\n",
    "      ##mfcc_len: MFCC coefficient length\n",
    "      ##mean_signal_length: MFCC feature average length\n",
    "  \t  ##\"\"\"\n",
    "      \n",
    "    signal, fs = librosa.load(file_path)\n",
    "    s_len = len(signal)\n",
    "\n",
    "    if version== \"RAVDE\":\n",
    "        mean_signal_length =110000\n",
    "    if version == \"IEMOCAP\":\n",
    "        mean_signal_length = 310000\n",
    "\n",
    "    if s_len < mean_signal_length:\n",
    "        pad_len = mean_signal_length - s_len\n",
    "        pad_rem = pad_len % 2\n",
    "        pad_len //= 2\n",
    "        signal = np.pad(signal, (pad_len, pad_len + pad_rem), 'constant', constant_values = 0)\n",
    "    else:\n",
    "        pad_len = s_len - mean_signal_length\n",
    "        pad_len //= 2\n",
    "        signal = signal[pad_len:pad_len + mean_signal_length]\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=fs, n_mfcc=39)\n",
    "    mfcc = mfcc.T\n",
    "    feature = mfcc\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_fromsignal(signal , fs,version, mfcc_len: int = 39,):\n",
    "  \t  ## \"\"\"\n",
    "      ## file_path: Speech signal folder\n",
    "      ##mfcc_len: MFCC coefficient length\n",
    "      ##mean_signal_length: MFCC feature average length\n",
    "  \t  ##\"\"\"\n",
    "      \n",
    "    s_len = len(signal)\n",
    "    if version== \"RAVDE\":\n",
    "        mean_signal_length =110000\n",
    "    if version == \"IEMOCAP\":\n",
    "        mean_signal_length = 310000\n",
    "\n",
    "    if s_len < mean_signal_length:\n",
    "        pad_len = mean_signal_length - s_len\n",
    "        pad_rem = pad_len % 2\n",
    "        pad_len //= 2\n",
    "        signal = np.pad(signal, (pad_len, pad_len + pad_rem), 'constant', constant_values = 0)\n",
    "    else:\n",
    "        pad_len = s_len - mean_signal_length\n",
    "        pad_len //= 2\n",
    "        signal = signal[pad_len:pad_len + mean_signal_length]\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=fs, n_mfcc=39)\n",
    "    mfcc = mfcc.T\n",
    "    feature = mfcc\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_fromframes(y,sr,frame_length,skip_length,version):\n",
    "    frames, sr = get_framed(y,sr,frame_length,skip_length)\n",
    "    mfccs = [get_feature_fromsignal(frame,sr,version) for frame in frames]   \n",
    "    mfccs_np = np.stack(mfccs) \n",
    "    return mfccs_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_audios(audio1,audio2,combined_audio_name):\n",
    "\n",
    "    y1, sr1 = librosa.load(audio1)\n",
    "    y2, sr2 = librosa.load(audio2)\n",
    "\n",
    "    assert sr1 == sr2, \"Sample rates (sr) must be the same to concatenate the audios.\"\n",
    "\n",
    "    y = np.concatenate([y1, y2])\n",
    "    sf.write(combined_audio_name, y, sr1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker_order_from_duration(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    speaker_order = []\n",
    "    for line in lines:\n",
    "        _, speaker = line.split(\",\")\n",
    "        speaker_order.append(speaker.strip()) \n",
    "    \n",
    "    return speaker_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_multi_speakers(audiopath, durations_file, num_speakers):\n",
    "\n",
    "    y,sr = librosa.load(audiopath)\n",
    "    speakers_segments = {f'speaker{i+1}': [] for i in range(num_speakers)} #dictionary of speakers and their parts.\n",
    "\n",
    "    with open(durations_file, 'r') as file:\n",
    "        for line in file:\n",
    "            time_range, speaker = line.strip().split(' , ')\n",
    "\n",
    "            start_time_str, end_time_str = time_range.split(' - ')\n",
    "            start_min, start_sec = map(int, start_time_str.split(':'))\n",
    "            end_min, end_sec = map(int, end_time_str.split(':'))\n",
    "\n",
    "            start_sample = ((start_min * 60) + start_sec) * sr\n",
    "            end_sample = ((end_min * 60) + end_sec) * sr\n",
    "\n",
    "            segment = y[start_sample:end_sample]\n",
    "\n",
    "            if speaker.strip() in speakers_segments:\n",
    "                speakers_segments[speaker.strip()].append(segment)\n",
    "    \n",
    "    return speakers_segments, sr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_two_speakers(audio_path, durations_file):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    speaker1_segments = []\n",
    "    speaker2_segments = []\n",
    "\n",
    "    with open(durations_file, 'r') as file:\n",
    "        for line in file:\n",
    "            time_range, speaker = line.strip().split(' , ')\n",
    "\n",
    "            start_time_str, end_time_str = time_range.split(' - ')\n",
    "            start_min, start_sec = map(int, start_time_str.split(':'))\n",
    "            end_min, end_sec = map(int, end_time_str.split(':'))\n",
    "\n",
    "            start_sample = ((start_min * 60) + start_sec) * sr\n",
    "            end_sample = ((end_min * 60) + end_sec) * sr\n",
    "\n",
    "            segment = y[start_sample:end_sample]\n",
    "\n",
    "            if speaker.strip() == 'speaker1':\n",
    "                speaker1_segments.append(segment)\n",
    "            else:\n",
    "                speaker2_segments.append(segment)\n",
    "\n",
    "    # speaker1_audio = np.concatenate(speaker1_segments)\n",
    "    # speaker2_audio = np.concatenate(speaker2_segments)\n",
    "\n",
    "    # output_file_speaker1 = audio_path.split('.')[0] + '_speaker1.wav'\n",
    "    # output_file_speaker2 = audio_path.split('.')[0] + '_speaker2.wav'\n",
    "\n",
    "    # sf.write(output_file_speaker1, speaker1_audio, sr)\n",
    "    # sf.write(output_file_speaker2, speaker2_audio, sr)\n",
    "\n",
    "    return speaker1_segments, speaker2_segments ,sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_speakers_static(signalpath, duration):  #use only if two speakers speak for exactly same duration. For example if each speaker speaks for 8 seconds in an alternating way, then duration is 8.\n",
    "    \n",
    "    y, sr = librosa.load(signalpath)\n",
    "\n",
    "    num_samples_8_sec = sr * duration\n",
    "\n",
    "    speaker1_samples = []\n",
    "    speaker2_samples = []\n",
    "\n",
    "    for idx, start_sample in enumerate(range(0, len(y), num_samples_8_sec)):\n",
    "        end_sample = start_sample + num_samples_8_sec\n",
    "\n",
    "        slice = y[start_sample:end_sample]\n",
    "\n",
    "        if idx % 2 == 0:\n",
    "            speaker1_samples.append(slice)\n",
    "        else:\n",
    "            speaker2_samples.append(slice)\n",
    "\n",
    "    #uncomment if want to download the speaker audios as one. \n",
    "   # speaker1_audio = np.concatenate(speaker1_samples)\n",
    "   #  speaker2_audio = np.concatenate(speaker2_samples)\n",
    "\n",
    "   # filename_base = os.path.splitext(os.path.basename(signalpath))[0]\n",
    "\n",
    "    # speaker1_output = filename_base + '_speaker1.wav'\n",
    "    # speaker2_output = filename_base + '_speaker2.wav'\n",
    "\n",
    "    # sf.write(speaker1_output, speaker1_audio, sr)\n",
    "    # sf.write(speaker2_output, speaker2_audio, sr)\n",
    "    \n",
    "     \n",
    "    return speaker1_samples, speaker2_samples,sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_twospeaker_predictions(class_labels, y_pred_best1, y_pred_best2):\n",
    "\n",
    "        transposed_probabilities1 = np.transpose(y_pred_best1)\n",
    "        transposed_probabilities2 = np.transpose(y_pred_best2)\n",
    "    \n",
    "        colors = [\"r\", \"c\", \"g\", \"y\", \"m\", \"b\", \"k\", \"orange\"]\n",
    "        marker = ['s', '^']  \n",
    "        lines = []\n",
    "        labels = []\n",
    "\n",
    "        plt.figure(figsize=(20,12))\n",
    "        for i, class_name in enumerate(class_labels):\n",
    "            line1, = plt.plot(np.arange(y_pred_best1.shape[0]), transposed_probabilities1[i], label=class_name, color=colors[i], marker=marker[0])\n",
    "            #line2, = plt.plot(np.arange(y_pred_best2.shape[0]), transposed_probabilities2[i], label=class_name, color=colors[i], marker=marker[1])    \n",
    "            line2, = plt.plot(np.arange(0.5, y_pred_best2.shape[0]+0.5), transposed_probabilities2[i], label=class_name, color=colors[i], marker=marker[1])  # Adjusted for speaker2\n",
    "            if i == 0:  \n",
    "                lines.append(line1)\n",
    "                lines.append(line2)\n",
    "                labels.append('Speaker1')\n",
    "                labels.append('Speaker2')\n",
    "\n",
    "        plt.xlabel('Sample index')\n",
    "        plt.ylabel('Probabilities')\n",
    "\n",
    "        leg1 = plt.legend(lines, labels, title='Speakers', loc='upper left')\n",
    "\n",
    "        emotions = [plt.Line2D([0], [0], color=c, lw=4) for c in colors]\n",
    "        leg2 = plt.legend(emotions, class_labels, title='Emotions', loc='lower left')\n",
    "\n",
    "        plt.gca().add_artist(leg1)  \n",
    "        plt.title('Emotion Probabilities for Each Class')\n",
    "        plt.savefig('output.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multispeaker_predictions(class_labels, y_preds_list, duration_order):\n",
    "        colors = [\"r\", \"c\", \"g\", \"y\", \"m\", \"b\", \"k\", \"orange\"]\n",
    "        marker = ['s', '^', 'o', 'D', '*', 'p']\n",
    "        lines = []\n",
    "        labels = []\n",
    "\n",
    "        plt.figure(figsize=(20, 12))\n",
    "\n",
    "        current_x_position = 0  \n",
    "\n",
    "        # dictionary maps each unique speaker to a marker\n",
    "        unique_speakers = list(set(duration_order))\n",
    "        speaker_to_marker = {speaker: marker[i % len(marker)] for i, speaker in enumerate(unique_speakers)}\n",
    "\n",
    "        added_speakers = set()  # to track which speakers are added to the legend\n",
    "\n",
    "        for idx, y_pred in enumerate(y_preds_list):\n",
    "            transposed_probabilities = np.transpose(y_pred)\n",
    "            current_speaker = duration_order[idx]  # Get the speaker from the duration order\n",
    "            current_marker = speaker_to_marker[current_speaker]  # Get the marker for the speaker\n",
    "\n",
    "            for i, class_name in enumerate(class_labels):\n",
    "                x_values = np.arange(current_x_position, current_x_position + len(y_pred))\n",
    "                line, = plt.plot(x_values, transposed_probabilities[i], label=class_name if idx == 0 else \"\", color=colors[i], marker=current_marker)\n",
    "                \n",
    "                if i == 0 and current_speaker not in added_speakers:\n",
    "                    lines.append(line)\n",
    "                    labels.append(current_speaker)\n",
    "                    added_speakers.add(current_speaker)\n",
    "\n",
    "            current_x_position += len(y_pred)  # Increase the x_position for the next speaker\n",
    "\n",
    "        plt.xlabel('Sample index')\n",
    "        plt.ylabel('Probabilities')\n",
    "\n",
    "        leg1 = plt.legend(lines, labels, title='Speakers', loc='upper left')  #legend for speaker markers\n",
    "\n",
    "        emotions = [plt.Line2D([0], [0], color=c, lw=4) for c in colors]  #legend for emotions\n",
    "        leg2 = plt.legend(emotions, class_labels, title='Emotions', loc='lower left')\n",
    "\n",
    "        plt.gca().add_artist(leg1)\n",
    "        plt.title('Emotion Probabilities for Each Class')\n",
    "        plt.savefig('output.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_speakers_from_duration_file(file_path):\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    speakers = set()\n",
    "    \n",
    "    for line in lines:\n",
    "        _, speaker = line.split(',')\n",
    "        speakers.add(speaker.strip())\n",
    "    \n",
    "    return len(speakers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(audio_file ,duration_file,model_type=1,fold=8):\n",
    "  \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--mode', type=str, default=\"test\")\n",
    "    parser.add_argument('--model_path', type=str, default='./Models/')\n",
    "    parser.add_argument('--result_path', type=str, default='./Results/')\n",
    "    parser.add_argument('--test_path', type=str, default='./Test_Models/EMODB_46')\n",
    "    parser.add_argument('--data', type=str, default='RAVDE')\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--beta1', type=float, default=0.93)\n",
    "    parser.add_argument('--beta2', type=float, default=0.98)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--epoch', type=int, default=60)\n",
    "    parser.add_argument('--dropout', type=float, default=0.1)\n",
    "    parser.add_argument('--random_seed', type=int, default=46)\n",
    "    parser.add_argument('--activation', type=str, default='relu')\n",
    "    parser.add_argument('--filter_size', type=int, default=39)\n",
    "    parser.add_argument('--dilation_size', type=int, default=10)\n",
    "    parser.add_argument('--kernel_size', type=int, default=2)\n",
    "    parser.add_argument('--stack_size', type=int, default=1)\n",
    "    parser.add_argument('--split_fold', type=int, default=10)\n",
    "\n",
    "\n",
    "    dataset_paths = {  \"IEMOCAP\" : './Test_Models/IEMOCAP_16', \"RAVDE\" : './Test_Models/RAVDE_46',}\n",
    "\n",
    "    file = duration_file\n",
    "    record= audio_file\n",
    "    num_speakers = count_speakers_from_duration_file(file)\n",
    "\n",
    "    if model_type==1:\n",
    "        version = \"IEMOCAP\"\n",
    "        path ='./Test_Models/IEMOCAP_16'\n",
    "        frame_length = 10\n",
    "        skip_length = 4\n",
    "        args = parser.parse_args('--mode test --model_path ./Models/ --result_path ./Results/ --test_path ./Test_Models/IEMOCAP_16 --data IEMOCAP --lr 0.001 --beta1 0.93 --beta2 0.98 --batch_size 64 --epoch 60 --dropout 0.1 --random_seed 46 --activation relu --filter_size 39 --dilation_size 10 --kernel_size 2 --stack_size 1 --split_fold 10'.split())\n",
    "\n",
    "    if model_type==2:\n",
    "        version = \"RAVDE\"\n",
    "        path ='./Test_Models/RAVDE_46'\n",
    "        frame_length = 5\n",
    "        skip_length = 2.5\n",
    "        args = parser.parse_args('--mode test --model_path ./Models/ --result_path ./Results/ --test_path ./Test_Models/RAVDE_46 --data RAVDE --lr 0.001 --beta1 0.93 --beta2 0.98 --batch_size 64 --epoch 60 --dropout 0.1 --random_seed 46 --activation relu --filter_size 39 --dilation_size 8 --kernel_size 2 --stack_size 1 --split_fold 10'.split())\n",
    "        \n",
    "\n",
    "    if args.data==\"IEMOCAP\" and args.dilation_size!=10:\n",
    "        args.dilation_size = 10\n",
    "        print(\"IEMOCAP\")\n",
    "\n",
    "    data = np.load(\"./MFCC/\"+args.data+\".npy\",allow_pickle=True).item()\n",
    "    x_source = data[\"x\"]\n",
    "    y_source = data[\"y\"]\n",
    "\n",
    "    CLASS_LABELS_finetune = (\"angry\", \"fear\", \"happy\", \"neutral\",\"sad\")\n",
    "\n",
    "    RAVDE_CLASS_LABELS = (\"angry\", \"calm\", \"disgust\", \"fear\", \"happy\", \"neutral\",\"sad\",\"surprise\")#rav\n",
    "    IEMOCAP_CLASS_LABELS = (\"angry\", \"happy\", \"neutral\", \"sad\")#iemocap\n",
    "\n",
    "    CLASS_LABELS_dict = { \"IEMOCAP\": IEMOCAP_CLASS_LABELS, \"RAVDE\": RAVDE_CLASS_LABELS,  }\n",
    "    \n",
    "    CLASS_LABELS = CLASS_LABELS_dict[args.data]\n",
    "\n",
    "    model = TIMNET_Model(args=args, input_shape=x_source.shape[1:], class_label=CLASS_LABELS)\n",
    "\n",
    "\n",
    "    if(num_speakers ==1 ):\n",
    "\n",
    "        y ,sr = librosa.load(record)\n",
    "        mfcc = get_features_fromframes(y ,sr, frame_length, skip_length,version)\n",
    "        predictions = model.test_one_sample(mfcc, True,path,fold)\n",
    "\n",
    "    if(num_speakers ==2):\n",
    "        speaker1_segments,speaker2_segments, sr = seperate_two_speakers(record,file)\n",
    "        mfccs_speaker1 = []\n",
    "        mfccs_speaker2 = []\n",
    "        for seg in speaker1_segments:\n",
    "                mfcc_speaker1 = get_features_fromframes(seg,sr, frame_length, skip_length,version)\n",
    "                mfccs_speaker1.append(mfcc_speaker1)\n",
    "        for segment in speaker2_segments:\n",
    "                mfcc_speaker2 = get_features_fromframes(segment ,sr, frame_length, skip_length, version)\n",
    "                mfccs_speaker2.append(mfcc_speaker2)\n",
    "        mfccs_speaker1 = np.vstack(mfccs_speaker1)\n",
    "        mfccs_speaker2 = np.vstack(mfccs_speaker2) \n",
    "        y_pred_best1 , y_pred_best2 = model.test_two_samples(mfccs_speaker1, mfccs_speaker2,path,fold)\n",
    "        visualize_twospeaker_predictions(CLASS_LABELS, y_pred_best1 , y_pred_best2 )\n",
    "\n",
    "    else:\n",
    "        speakers_segments, sr = separate_multi_speakers(record,file, num_speakers)\n",
    "        all_speaker_mfccs = {}\n",
    "        j = 0\n",
    "        for speaker, segments in speakers_segments.items():\n",
    "            if speaker not in all_speaker_mfccs:\n",
    "                 all_speaker_mfccs[speaker] = []\n",
    "            for seg in segments:\n",
    "                mfcc = get_features_fromframes(seg,sr, frame_length, skip_length,version)\n",
    "                all_speaker_mfccs[speaker].append(mfcc) \n",
    "            j +=1\n",
    "        speaker_order= get_speaker_order_from_duration(file)\n",
    "        y_pred_list = model.test_multi_speaker(all_speaker_mfccs, speaker_order,path,fold)\n",
    "        visualize_multispeaker_predictions(CLASS_LABELS, y_pred_list,speaker_order)\n",
    "    return \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMNET MODEL SHAPE: (606, 39)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main(kavga,\u001b[39m'\u001b[39;49m\u001b[39mduration8sec.txt\u001b[39;49m\u001b[39m'\u001b[39;49m ,model_type\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, fold\u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[29], line 83\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(record, file, model_type, fold)\u001b[0m\n\u001b[1;32m     81\u001b[0m     mfccs_speaker1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack(mfccs_speaker1)\n\u001b[1;32m     82\u001b[0m     mfccs_speaker2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack(mfccs_speaker2) \n\u001b[0;32m---> 83\u001b[0m     y_pred_best1 , y_pred_best2 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtest_two_samples(mfccs_speaker1, mfccs_speaker2,path,fold)\n\u001b[1;32m     84\u001b[0m     visualize_twospeaker_predictions(CLASS_LABELS, y_pred_best1 , y_pred_best2 )\n\u001b[1;32m     86\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[35], line 119\u001b[0m, in \u001b[0;36mTIMNET_Model.test_two_samples\u001b[0;34m(self, sample1, sample2, path, fold)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_two_samples\u001b[39m(\u001b[39mself\u001b[39m, sample1, sample2,path,fold):\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_model()\n\u001b[1;32m    120\u001b[0m     weight_path \u001b[39m=\u001b[39m path\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39msplit_fold)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m-fold_weights_best_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(fold)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.hdf5\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_weights(weight_path)\n",
      "Cell \u001b[0;32mIn[35], line 47\u001b[0m, in \u001b[0;36mTIMNET_Model.create_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs\u001b[39m=\u001b[39mInput(shape \u001b[39m=\u001b[39;49m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_shape[\u001b[39m0\u001b[39;49m],\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_shape[\u001b[39m1\u001b[39;49m]))\n\u001b[1;32m     48\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_decision \u001b[39m=\u001b[39m TIMNET(nb_filters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mfilter_size,\n\u001b[1;32m     50\u001b[0m                             kernel_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mkernel_size, \n\u001b[1;32m     51\u001b[0m                             nb_stacks\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mstack_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m                             return_sequences\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[1;32m     56\u001b[0m                             name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTIMNET\u001b[39m\u001b[39m'\u001b[39m)(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs)\n",
      "File \u001b[0;32m~/Downloads/untitled2/.conda/enter/envs/TFenv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Downloads/untitled2/.conda/enter/envs/TFenv/lib/python3.10/site-packages/keras/engine/input_layer.py:438\u001b[0m, in \u001b[0;36mInput\u001b[0;34m(shape, batch_size, name, dtype, sparse, tensor, ragged, type_spec, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     input_layer_config\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m    436\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: batch_size, \u001b[39m\"\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m\"\u001b[39m: shape}\n\u001b[1;32m    437\u001b[0m     )\n\u001b[0;32m--> 438\u001b[0m input_layer \u001b[39m=\u001b[39m InputLayer(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minput_layer_config)\n\u001b[1;32m    440\u001b[0m \u001b[39m# Return tensor including `_keras_history`.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m# Note that in this case train_output and test_output are the same pointer.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m outputs \u001b[39m=\u001b[39m input_layer\u001b[39m.\u001b[39m_inbound_nodes[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39moutputs\n",
      "File \u001b[0;32m~/Downloads/untitled2/.conda/enter/envs/TFenv/lib/python3.10/site-packages/keras/engine/base_layer.py:770\u001b[0m, in \u001b[0;36mLayer.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    768\u001b[0m     auto_get_config \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 770\u001b[0m     instance\u001b[39m.\u001b[39m_auto_get_config \u001b[39m=\u001b[39m auto_get_config\n\u001b[1;32m    771\u001b[0m     \u001b[39mif\u001b[39;00m auto_get_config:\n\u001b[1;32m    772\u001b[0m         instance\u001b[39m.\u001b[39m_auto_config \u001b[39m=\u001b[39m generic_utils\u001b[39m.\u001b[39mConfig(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Downloads/untitled2/.conda/enter/envs/TFenv/lib/python3.10/site-packages/keras/engine/base_layer.py:3166\u001b[0m, in \u001b[0;36mLayer.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   3161\u001b[0m             value\u001b[39m.\u001b[39m_use_resource_variables \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3163\u001b[0m \u001b[39m# Append value to list of trainable / non-trainable weights if relevant\u001b[39;00m\n\u001b[1;32m   3164\u001b[0m \u001b[39m# TODO(b/125122625): This won't pick up on any variables added to a\u001b[39;00m\n\u001b[1;32m   3165\u001b[0m \u001b[39m# list/dict after creation.\u001b[39;00m\n\u001b[0;32m-> 3166\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_track_variables(value)\n\u001b[1;32m   3168\u001b[0m \u001b[39m# TODO(b/180760306) Skip the auto trackable from tf.Module to keep\u001b[39;00m\n\u001b[1;32m   3169\u001b[0m \u001b[39m# status quo. See the comment at __delattr__.\u001b[39;00m\n\u001b[1;32m   3170\u001b[0m \u001b[39msuper\u001b[39m(tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mtracking\u001b[39m.\u001b[39mAutoTrackable, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\n\u001b[1;32m   3171\u001b[0m     name, value\n\u001b[1;32m   3172\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/untitled2/.conda/enter/envs/TFenv/lib/python3.10/site-packages/keras/engine/base_layer.py:3182\u001b[0m, in \u001b[0;36mLayer._track_variables\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   3177\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m   3178\u001b[0m             trackable_obj, tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mtracking\u001b[39m.\u001b[39mTrackableDataStructure\n\u001b[1;32m   3179\u001b[0m         ):\n\u001b[1;32m   3180\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_track_variables(trackable_obj)\n\u001b[0;32m-> 3182\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_track_variables\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[1;32m   3183\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Tracks `Variable`s including `Variable`s in `CompositeTensor`s.\"\"\"\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m     \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(value):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(record = , duration_file =  ,model_type=1, fold= 5) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
